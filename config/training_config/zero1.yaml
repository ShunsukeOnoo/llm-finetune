training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 2
  num_train_epochs: 1
  dataloader_num_workers: 8
  fp16: true
  optim: "adamw_torch"
  learning_rate: 5.0e-5
  logging_steps: 1
  evaluation_strategy: "no"
  save_strategy: "steps"
  eval_steps: 100
  save_steps: 2000
  save_total_limit: 2
  deepspeed: configs/ds_configs/ds_config_zero1.json
  output_dir: ./output/
  report_to: "wandb"
